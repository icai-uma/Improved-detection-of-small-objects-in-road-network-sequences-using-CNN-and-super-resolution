{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9tsU1hqfDAa"
      },
      "source": [
        "# **IMPORTACIÓN DE LAS LIBRERÍAS NECESARIAS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QCBuW20fIJz"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCM8j4zGyMCR"
      },
      "source": [
        "# **FUNCIONES AUXILIARES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXe-njFayN2N"
      },
      "source": [
        "import os\n",
        "\n",
        "def create_dir(path):\n",
        "  try:\n",
        "      if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "      else:\n",
        "        shutil.rmtree(path, ignore_errors=True)\n",
        "        os.makedirs(path)\n",
        "  except OSError:\n",
        "      print ('Error creando la carpeta de las imágenes procesadas.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79U_xAPmc6XL"
      },
      "source": [
        "# **APLICACIÓN DE PROCESOS SR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "X_u8nEpeclvF",
        "outputId": "9947f68f-0869-47bb-fca9-1d6303a29db0"
      },
      "source": [
        "!pip install --upgrade opencv-contrib-python\n",
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencv-contrib-python\n",
            "  Using cached https://files.pythonhosted.org/packages/17/69/cc755e763f4bb20cb25c7696172e6bb556719faf458c5362f4e54d6cd765/opencv_contrib_python-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.19.5)\n",
            "Installing collected packages: opencv-contrib-python\n",
            "  Found existing installation: opencv-contrib-python 4.3.0.36\n",
            "    Uninstalling opencv-contrib-python-4.3.0.36:\n",
            "      Successfully uninstalled opencv-contrib-python-4.3.0.36\n",
            "Successfully installed opencv-contrib-python-4.5.1.48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "5wMP_E88RDtx",
        "outputId": "4f7348ef-4428-458c-b7da-4eb2b39f6edf"
      },
      "source": [
        "!pip install opencv-contrib-python==4.3.0.36"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencv-contrib-python==4.3.0.36\n",
            "  Using cached https://files.pythonhosted.org/packages/2b/3c/41808edb511fa70092bf440aeefcc9719d98b86174835e226933bc9958c4/opencv_contrib_python-4.3.0.36-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==4.3.0.36) (1.19.5)\n",
            "Installing collected packages: opencv-contrib-python\n",
            "  Found existing installation: opencv-contrib-python 4.5.1.48\n",
            "    Uninstalling opencv-contrib-python-4.5.1.48:\n",
            "      Successfully uninstalled opencv-contrib-python-4.5.1.48\n",
            "Successfully installed opencv-contrib-python-4.3.0.36\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "b3pVOBwtdKfK",
        "outputId": "29aa39ec-c2b4-4e23-d549-442da4e16f1b"
      },
      "source": [
        "import wget\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "def download_model_SR(model_name):\n",
        "    #FSRCNN-small_x2/3/4, FSRCNN_x2/3/4\n",
        "    data_dir = '/content/MODEL_SR'\n",
        "    create_dir(data_dir)\n",
        "    #base_url = 'http://mcml.yonsei.ac.kr/files/srzoo/'\n",
        "    base_url = 'https://github.com/Saafke/FSRCNN_Tensorflow/blob/master/models/'\n",
        "    model_file = base_url+ model_name +'.pb?raw=true'\n",
        "    download_model = \"/content/MODEL_SR/\"+model_name+'.pb'\n",
        "    wget.download(model_file, download_model)\n",
        "    return download_model\n",
        "\n",
        "def download_images(url_path):\n",
        "    data_dir = '/content/data/'\n",
        "    create_dir(data_dir)\n",
        "    download_image = '/content/data/butterfly1.jpg'\n",
        "    wget.download(url_path, download_image)\n",
        "    return download_image\n",
        "\n",
        "\n",
        "MODEL_NAME = 'FSRCNN_x2'\n",
        "PATH_TO_MODEL_DIR = download_model_SR(MODEL_NAME)\n",
        "IMAGE_PATH = 'https://www.sciencemag.org/sites/default/files/styles/article_main_large/public/butterfly_16x9_0.jpg?itok=jZ3DYvGK'\n",
        "download_images(IMAGE_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/data/butterfly1.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO6XVDuwccFB",
        "outputId": "656a63a7-f572-4d04-97ea-1ad496ca506a"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "from cv2 import dnn_superres\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "MODEL_NAME = 'FSRCNN_x2'\n",
        "PATH_TO_MODEL_DIR = download_model_SR(MODEL_NAME)\n",
        "\n",
        "data_dir = '/content/TensorFlow/DATAPROCESS/'\n",
        "create_dir(data_dir)\n",
        "print(PATH_TO_MODEL_DIR)\n",
        "\n",
        "# Create an SR object\n",
        "sr = dnn_superres.DnnSuperResImpl_create()\n",
        "# Read image\n",
        "image = cv2.imread('/content/data/butterfly1.jpg')\n",
        "# Read the desired model EDSR\n",
        "sr.readModel(PATH_TO_MODEL_DIR)\n",
        "# Set the desired model and scale to get correct pre- and post-processing\n",
        "sr.setModel(\"fsrcnn\", 2)\n",
        "result = sr.upsample(image)\n",
        "# Save the image\n",
        "cv2.imwrite('/content/data/butterfly2.jpg', result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MODEL_SR/FSRCNN_x2.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh01GZxf8X7W"
      },
      "source": [
        "def start_SR_1():\n",
        "  sr = dnn_superres.DnnSuperResImpl_create()\n",
        "  sr.readModel(PATH_TO_MODEL_DIR)\n",
        "  sr.setModel(\"fsrcnn\", 2)\n",
        "  return sr\n",
        "\n",
        "def start_SR(PATH):\n",
        "  sr = dnn_superres.DnnSuperResImpl_create()\n",
        "  sr.readModel(PATH)\n",
        "  sr.setModel(\"fsrcnn\", 2)\n",
        "  return sr\n",
        "\n",
        "def start_SR_MOD(PATH):\n",
        "  sr = dnn_superres.DnnSuperResImpl_create()\n",
        "  sr.readModel(PATH)\n",
        "  sr.setModel(\"fsrcnn\", 4)\n",
        "  return sr\n",
        "\n",
        "\n",
        "def apply_SR(sr,image,image_process_path):\n",
        "  image = cv2.imread(image)\n",
        "  result = sr.upsample(image)\n",
        "  cv2.imwrite(image_process_path, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgSjdOfCjYGN"
      },
      "source": [
        "# **CONOCER EL TAMAÑO DE LA IMAGEN TRAS SU PROCESAMIENTO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovJjB9IOjbY-",
        "outputId": "e62d3861-7d40-4181-fa89-169fec51ed5f"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def know_dimensions(path):\n",
        "  img = cv2.imread(path)\n",
        "  height, width, channels = img.shape\n",
        "  print(height, width, channels)\n",
        "\n",
        "IMAGE_PATH_VANILLA = '/content/data/butterfly1.jpg'\n",
        "know_dimensions(IMAGE_PATH_VANILLA)\n",
        "\n",
        "IMAGE_PATH_SR = '/content/data/butterfly2.jpg'\n",
        "know_dimensions(IMAGE_PATH_SR)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "720 1280 3\n",
            "1440 2560 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0uzUQ_iskU9"
      },
      "source": [
        "# **APLICACIÓN DEL MODELO DE DETECCIÓN DE OBJETOS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciGQfMZZtxem",
        "outputId": "08a234ef-b8d4-4e93-bf01-3d84bd477261"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import wget\n",
        "import shutil\n",
        "\n",
        "path = \"/content/TensorFlow/\"\n",
        "path_instalation = \"/content/TensorFlow/master.zip\"\n",
        "try:\n",
        "    shutil.rmtree(path, ignore_errors=True)\n",
        "    os.mkdir(path)\n",
        "    path_download = \"https://github.com/tensorflow/models/archive/master.zip\"\n",
        "    wget.download(path_download, path_instalation)\n",
        "    fantasy_zip = zipfile.ZipFile(path_instalation)\n",
        "    fantasy_zip.extractall(path)\n",
        "    fantasy_zip.close()\n",
        "    os.rename(\"/content/TensorFlow/models-master\", \"/content/TensorFlow/models\")\n",
        "    os.remove(path_instalation)\n",
        "except OSError:\n",
        "    print (\"El directorio que se pretende borrar no existe\" % path)\n",
        "else:\n",
        "    print (\"Directorio de trabajo generado correctamente %s \" % path)\n",
        "\n",
        "os.chdir(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directorio de trabajo generado correctamente /content/TensorFlow/ \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2oKrAfUuEsx",
        "outputId": "2a382ba2-ea2a-4178-9633-12f8fd3ed8e8"
      },
      "source": [
        "cd \"/content/TensorFlow/models/research/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TensorFlow/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDXgWW7CuJ2_"
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSHaJkRXuPC1"
      },
      "source": [
        "# **Install the Object Detection API**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68DMvAdEuStJ",
        "outputId": "7a2c15b5-f637-4e8c-f915-c74508bca4aa"
      },
      "source": [
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/TensorFlow/models/research\n",
            "Requirement already satisfied: avro-python3 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.10.2)\n",
            "Requirement already satisfied: apache-beam in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.29.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.22)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.2)\n",
            "Requirement already satisfied: lvis in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.5)\n",
            "Requirement already satisfied: tf-models-official in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.4.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.12.4)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (2.6.0)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.0.0)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (2.8.1)\n",
            "Requirement already satisfied: numpy<1.21.0,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.19.5)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (0.3.1.1)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: fastavro<2,>=0.21.4 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.4.0)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.7.4.3)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (2.25.1)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (0.18.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools->object-detection==0.1) (56.0.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (4.5.1.48)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (1.12.8)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (1.5.12)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (1.21.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (0.12.1)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (8.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (0.1.95)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (0.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (5.4.1)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (4.7.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (1.28.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (1.26.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official->object-detection==0.1) (0.22.2.post1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official->object-detection==0.1) (0.1.6)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (2.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (2.4.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.12.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (2.10.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (20.3.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (0.29.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (5.1.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official->object-detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (4.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official->object-detection==0.1) (20.9)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (3.3.4)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tf-models-official->object-detection==0.1) (3.4.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (3.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->tf-models-official->object-detection==0.1) (3.1.0)\n",
            "Building wheels for collected packages: object-detection\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-cp37-none-any.whl size=1647891 sha256=fac94fa0b83ae2b0abcb3a45c77c77c6737dee64a402376585e95837b73fb1a8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qp59wy2b/wheels/1e/a6/ce/339480921228eb089bee0cdaf7efbaa7b5cf803ec6420ab905\n",
            "Successfully built object-detection\n",
            "Installing collected packages: object-detection\n",
            "  Found existing installation: object-detection 0.1\n",
            "    Uninstalling object-detection-0.1:\n",
            "      Successfully uninstalled object-detection-0.1\n",
            "Successfully installed object-detection-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udqmtb-MvYKH"
      },
      "source": [
        "# **GENERACIÓN DEL ESPACIO EN EL CUAL SE ALMACENARÁN LAS IMÁGENES A INFERIR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq-hMI6Dvd9B"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import wget\n",
        "import shutil\n",
        "\n",
        "path = \"/content/TensorFlow/DATA\"\n",
        "create_dir(path)\n",
        "os.chdir(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUZx3m2FzgW3"
      },
      "source": [
        "# **ACTIVAR ASIGNACIÓN DINÁMICA DE MEMORIA GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoL_AJRQzjPC"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "# Enable GPU dynamic memory allocation\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1G_PvGSznjz"
      },
      "source": [
        "# **DESCARGA DEL MODELO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-zGz0YVzrz6"
      },
      "source": [
        "def download_model(model_name, model_date):\n",
        "    base_url = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
        "    model_file = model_name + '.tar.gz'\n",
        "    model_dir = tf.keras.utils.get_file(fname=model_name,\n",
        "                                        origin=base_url + model_date + '/' +\n",
        "                                        model_file,\n",
        "                                        untar=True)\n",
        "    return str(model_dir)\n",
        "\n",
        "MODEL_DATE = '20200711'\n",
        "MODEL_NAME = 'efficientdet_d4_coco17_tpu-32'\n",
        "\n",
        "\n",
        "\n",
        "PATH_TO_MODEL_DIR = download_model(MODEL_NAME, MODEL_DATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2unFFjxzucC"
      },
      "source": [
        "# **DESCARGA LAS ETIQUETAS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pd13zO6zyS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0aedc4-f6ff-4fc3-f558-6402e723c73e"
      },
      "source": [
        "import pathlib\n",
        "def download_labels(filename):\n",
        "    base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
        "    label_dir = tf.keras.utils.get_file(fname=filename,\n",
        "                                        origin=base_url + filename,\n",
        "                                        untar=False)\n",
        "    label_dir = pathlib.Path(label_dir)\n",
        "    return str(label_dir)\n",
        "\n",
        "LABEL2_FILENAME = 'kitti_label_map.pbtxt'\n",
        "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
        "LABEL_FILENAME2 = 'mscoco_complete_label_map.pbtxt'\n",
        "PATH_TO_LABELS = download_labels(LABEL_FILENAME2)\n",
        "print(PATH_TO_LABELS)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets/mscoco_complete_label_map.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAnHCtlVz24I"
      },
      "source": [
        "# **CARGA DEL MODELO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTfso_TKz5u7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9091483d-c50c-4257-90ec-0f8603545048"
      },
      "source": [
        "import time\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
        "print('Loading model...', end='')\n",
        "start_time = time.time()\n",
        "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Done! Took {} seconds'.format(elapsed_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_194916) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_167775) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_131153) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_134709) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference___call___46929) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_187495) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_EfficientDet-D4_layer_call_and_return_conditional_losses_175196) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done! Took 70.63821983337402 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZzILFC_0AWO"
      },
      "source": [
        "# **CARGA LAS ETIQUETAS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoWeygg00CYf"
      },
      "source": [
        "category_index = label_map_util.create_category_index_from_labelmap(\n",
        "    PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzt0v0xz0Sm-"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADlei5WDfEr9"
      },
      "source": [
        "# **EVALUACIÓN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGiDg5HLfqEb"
      },
      "source": [
        "# **DETECTOR DE ELEMENTOS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI2S8UWTfsrM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c29b86f3-8a83-492f-a109-9f2acc6ef898"
      },
      "source": [
        "import numpy as np\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.ticker as plticker\n",
        "import tensorflow as tf\n",
        "import statistics\n",
        "import time\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "import shutil\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "def make_inference45(image_path,counter,image_save,etiquetas):\n",
        "  '''\n",
        "  print('Running inference for {}... '.format(image_path), end='')\n",
        "  '''\n",
        "\n",
        "  image_np = load_image_into_numpy_array(image_path)\n",
        "  input_tensor = tf.convert_to_tensor(image_np)\n",
        "  input_tensor = input_tensor[tf.newaxis, ...]\n",
        "  detections = detect_fn(input_tensor)\n",
        "  num_detections = int(detections.pop('num_detections'))\n",
        "  detections = {key: value[0, :num_detections].numpy()\n",
        "                   for key, value in detections.items()}\n",
        "  detections['num_detections'] = num_detections\n",
        "  detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "  image_np_with_detections = image_np.copy()\n",
        "\n",
        "\n",
        "  boxes_filter = []\n",
        "  scores_filter = []\n",
        "  classes_filter = []\n",
        "\n",
        "  for item in range(len(detections['detection_boxes'])):\n",
        "    if detections['detection_classes'][item] in etiquetas:\n",
        "      boxes_filter.append(detections['detection_boxes'][item].tolist())\n",
        "      scores_filter.append(detections['detection_scores'][item])\n",
        "      classes_filter.append(detections['detection_classes'][item])\n",
        "\n",
        "\n",
        "  detections['detection_boxes'] = np.array(boxes_filter)\n",
        "  detections['detection_classes'] = np.array(classes_filter)\n",
        "  detections['detection_scores'] = np.array(scores_filter)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_detections,\n",
        "        detections['detection_boxes'],\n",
        "        detections['detection_classes'],\n",
        "        detections['detection_scores'],\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        max_boxes_to_draw=100,\n",
        "        min_score_thresh=.3,\n",
        "        agnostic_mode=False,\n",
        "        line_thickness=1)\n",
        "  plt.axis('off')\n",
        "\n",
        "  nombre = counter\n",
        "  plt.imshow(image_np_with_detections)\n",
        "  plt.savefig(image_save+nombre,  dpi=dpi ,bbox_inches='tight',pad_inches = 0)\n",
        "\n",
        "  plt.clf()\n",
        "\n",
        "  boxes = detections['detection_boxes']\n",
        "  scores = detections['detection_scores'],\n",
        "  clases_detected = detections['detection_classes']\n",
        "  img = cv2.imread(image_path)\n",
        "\n",
        "\n",
        "\n",
        "  boxes = detections['detection_boxes']\n",
        "  scores = detections['detection_scores'],\n",
        "  clases_detected = detections['detection_classes']\n",
        "  #print(detections['detection_boxes'])\n",
        "  min_score_thresh = .3\n",
        "  true_boxes  = boxes[scores[0] > min_score_thresh]\n",
        "  true_scores = scores[0][scores[0] > min_score_thresh]\n",
        "  true_clases = clases_detected[scores[0] > min_score_thresh]\n",
        "\n",
        "  salida = []\n",
        "  salida.append(true_boxes.tolist())\n",
        "  salida.append(true_scores.tolist())\n",
        "  salida.append(true_clases.tolist())\n",
        "\n",
        "  image = Image.open(image_path)\n",
        "  width, height = image.size\n",
        "\n",
        "  salida.append(width)\n",
        "  salida.append(height)\n",
        "\n",
        "  return salida\n",
        "'''\n",
        "IMAGE_PATHS_GOOD = natsorted(IMAGE_PATHS)\n",
        "counter = 0\n",
        "create_dir('/content/RESULTADOS')\n",
        "'''\n",
        "dpi = 300\n",
        "minimo = 0\n",
        "'''\n",
        "maximo = len(IMAGE_PATHS_GOOD)\n",
        "tiempos = []\n",
        "pruebas = list(range(minimo, maximo))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmaximo = len(IMAGE_PATHS_GOOD)\\ntiempos = []\\npruebas = list(range(minimo, maximo))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HamuMpHYPGu9"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/instances_default.json'\n",
        "diccionario_ids = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data['images']:\n",
        "        data_s_image = p3['file_name']\n",
        "        data_s_id = p3['id']\n",
        "        diccionario_ids[data_s_image] = data_s_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d42xqN-I-zsO"
      },
      "source": [
        "create_dir('/content/AUX')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH4txjBhPCO4"
      },
      "source": [
        "make_inference240('/content/229.jpg','0','/content/RESULTADOS_SR/',[3,4,6,8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2uxnxc-PD6k"
      },
      "source": [
        "make_inference45('/content/foto/110.jpg','1.jpg','/content/RESULTADOS/',[3,4,6,8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDIA78zgfYJT"
      },
      "source": [
        "# **CREACIÓN DE LAS ANOTACIONES CON EL MODELO BASE CENTERNET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqEpfC6E0FDR"
      },
      "source": [
        "import re\n",
        "imagenes_dir2 = '/content/foto'\n",
        "contenido2 = os.listdir(imagenes_dir2)\n",
        "imagenes_dir2 = []\n",
        "counter = 0\n",
        "result = []\n",
        "from time import time\n",
        "tiempos_procesar = []\n",
        "\n",
        "import json\n",
        "\n",
        "out = []\n",
        "concon = 0\n",
        "for fichero2 in contenido2:\n",
        "  print(concon)\n",
        "  concon = concon +1\n",
        "  x = re.search(\"[1-9]+[0-9]*\", fichero2)\n",
        "  id = x.group(0)\n",
        "  print(id)\n",
        "  image_path_gg = \"/content/foto/{}\".format(fichero2)\n",
        "  start_time = time()\n",
        "  salida = make_inference45(image_path_gg,fichero2,'/content/RESULTADOS/',[3,4,6,8])\n",
        "  elapsed_time = time() - start_time\n",
        "  tiempos_procesar.append(elapsed_time)\n",
        "  result = []\n",
        "  converted_num = int(id)\n",
        "  converted_num = diccionario_ids[fichero2]\n",
        "  print(fichero2, \" \",converted_num)\n",
        "\n",
        "  width = salida[3]\n",
        "  height = salida[4]\n",
        "\n",
        "  boxes2 = []\n",
        "\n",
        "  for box in salida[0]:\n",
        "    ymin = int(box[0]*height)\n",
        "    xmin = int(box[1]*width)\n",
        "    ymax = int(box[2]*height)\n",
        "    xmax = int(box[3]*width)\n",
        "\n",
        "\n",
        "    box_new = []\n",
        "    box_new.append(xmin)\n",
        "    box_new.append(ymin)\n",
        "    box_new.append(xmax-xmin)\n",
        "    box_new.append(ymax-ymin)\n",
        "\n",
        "\n",
        "    boxes2.append(box_new)\n",
        "\n",
        "\n",
        "\n",
        "  result.extend(\n",
        "        [\n",
        "            {\n",
        "                \"image_id\": converted_num,\n",
        "                \"category_id\": salida[2][k],\n",
        "                \"bbox\": box,\n",
        "                \"score\": salida[1][k],\n",
        "            }\n",
        "            for k, box in enumerate(boxes2)\n",
        "        ]\n",
        "  )\n",
        "\n",
        "  for sample in result:\n",
        "        out.append(sample)#detection result including\n",
        "\n",
        "with open('/content/test_data_normal.json', 'w', encoding='utf-8') as file:\n",
        "  json.dump(out, file, ensure_ascii=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAW0id-wxENW"
      },
      "source": [
        "print(tiempos_procesar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCEOJzuTO602"
      },
      "source": [
        "!zip -r '/content/RESULTADOS.zip' '/content/RESULTADOS'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXb1OF1Rfe-K"
      },
      "source": [
        "# **EVALUACIÓN DE LAS ANOTACIONES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELid6FnOsx5M"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import pylab\n",
        "import json\n",
        "\n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "\n",
        "\n",
        "annType = ['segm','bbox','keypoints']\n",
        "annType = annType[1]      #specify type here\n",
        "prefix = 'person_keypoints' if annType=='keypoints' else 'instances'\n",
        "print ('Running demo for *%s* results.'%(annType))\n",
        "\n",
        "# use the valadation labelme file\n",
        "# annFile = '/content/imagenesnormal.json'\n",
        "annFile = '/content/instances_default.json'\n",
        "cocoGt=COCO(annFile)\n",
        "#initialize COCO detections api\n",
        "# use the generated results\n",
        "resFile = '/content/test_data_normal.json'\n",
        "cocoDt=cocoGt.loadRes(resFile)\n",
        "\n",
        "\n",
        "dts = json.load(open(resFile,'r'))\n",
        "imgIds = [imid['image_id'] for imid in dts]\n",
        "imgIds = sorted(list(set(imgIds)))\n",
        "\n",
        "\n",
        "'''\n",
        "imgIds=sorted(cocoGt.getImgIds())\n",
        "imgIds=imgIds[0:24]\n",
        "imgId = imgIds[np.random.randint(24)]\n",
        "'''\n",
        "\n",
        "# running box evaluation\n",
        "cocoEval = COCOeval(cocoGt,cocoDt,annType)\n",
        "\n",
        "\n",
        "cocoEval.params.imgIds  = imgIds\n",
        "\n",
        "cocoEval.params.catIds = [3] # 1 stands for the 'person' class, you can increase or decrease the category as needed\n",
        "\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0_EOZJ_f0tz"
      },
      "source": [
        "# **---**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3daHtPbIDai"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import pylab\n",
        "import json\n",
        "\n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "\n",
        "\n",
        "annType = ['segm','bbox','keypoints']\n",
        "annType = annType[1]      #specify type here\n",
        "prefix = 'person_keypoints' if annType=='keypoints' else 'instances'\n",
        "print ('Running demo for *%s* results.'%(annType))\n",
        "\n",
        "# use the valadation labelme file\n",
        "# annFile = '/content/imagenesnormal.json'\n",
        "annFile = '/content/instances_default.json'\n",
        "cocoGt=COCO(annFile)\n",
        "#initialize COCO detections api\n",
        "# use the generated results\n",
        "resFile = '/content/test_data_MOD.json'\n",
        "cocoDt=cocoGt.loadRes(resFile)\n",
        "\n",
        "\n",
        "dts = json.load(open(resFile,'r'))\n",
        "imgIds = [imid['image_id'] for imid in dts]\n",
        "imgIds = sorted(list(set(imgIds)))\n",
        "\n",
        "\n",
        "'''\n",
        "imgIds=sorted(cocoGt.getImgIds())\n",
        "imgIds=imgIds[0:24]\n",
        "imgId = imgIds[np.random.randint(24)]\n",
        "'''\n",
        "\n",
        "# running box evaluation\n",
        "cocoEval = COCOeval(cocoGt,cocoDt,annType)\n",
        "\n",
        "\n",
        "cocoEval.params.imgIds  = imgIds\n",
        "\n",
        "cocoEval.params.catIds = [3] # 1 stands for the 'person' class, you can increase or decrease the category as needed\n",
        "\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjO1yejsf6X2"
      },
      "source": [
        "# **---**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLYscaeyf9AD"
      },
      "source": [
        "# **---**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHDIdTaBpjis"
      },
      "source": [
        "# **Enfoque propuesto**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGOHqhIz6wku"
      },
      "source": [
        "# **GENERACIÓN DE LAS IMÁGENES TRAS APLICAR SR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX4A0jfE5mPL"
      },
      "source": [
        "# **FUNCIÓN AUXILIAR PARA INTRODUCIR PADDING A LAS IMÁGENES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er7TOVTsirF5"
      },
      "source": [
        "import wget\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "def download_model_SR(model_name):\n",
        "    #FSRCNN-small_x2/3/4, FSRCNN_x2/3/4\n",
        "    data_dir = '/content/MODEL_SR'\n",
        "    create_dir(data_dir)\n",
        "    #base_url = 'http://mcml.yonsei.ac.kr/files/srzoo/'\n",
        "    base_url = 'https://github.com/Saafke/FSRCNN_Tensorflow/blob/master/models/'\n",
        "    model_file = base_url+ model_name +'.pb?raw=true'\n",
        "    download_model = \"/content/MODEL_SR/\"+model_name+'.pb'\n",
        "    wget.download(model_file, download_model)\n",
        "    return download_model\n",
        "\n",
        "def download_images(url_path):\n",
        "    data_dir = '/content/data/'\n",
        "    create_dir(data_dir)\n",
        "    download_image = '/content/data/butterfly1.jpg'\n",
        "    wget.download(url_path, download_image)\n",
        "    return download_image\n",
        "\n",
        "\n",
        "MODEL_NAME = 'FSRCNN_x2'\n",
        "PATH_TO_MODEL_DIR = download_model_SR(MODEL_NAME)\n",
        "IMAGE_PATH = 'https://www.sciencemag.org/sites/default/files/styles/article_main_large/public/butterfly_16x9_0.jpg?itok=jZ3DYvGK'\n",
        "download_images(IMAGE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNvlkezHj-zz"
      },
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def createSR(image_path):\n",
        "  IMAGE_ORIGINAL_PATH = image_path\n",
        "  IMAGE_SR_PATH = '/content/TensorFlow/DATAPROCESS/0_SR.png'\n",
        "  PATH_TO_MODEL_DIR = '/content/MODEL_SR/FSRCNN_x2.pb'\n",
        "  #A LA IMAGEN ORIGINAL LE VOY A APLICAR UN AUMENTO DE X2\n",
        "  sr = dnn_superres.DnnSuperResImpl_create()\n",
        "  image = cv2.imread(IMAGE_ORIGINAL_PATH)\n",
        "  sr.readModel(PATH_TO_MODEL_DIR)\n",
        "  sr.setModel(\"fsrcnn\", 2)\n",
        "  result = sr.upsample(image)\n",
        "  cv2.imwrite(IMAGE_SR_PATH, result)\n",
        "  img = cv.imread('/content/TensorFlow/DATAPROCESS/0_SR.png')\n",
        "  dst = cv.fastNlMeansDenoisingColored(img,None,8,8,7,11)\n",
        "  cv2.imwrite('/content/TensorFlow/DATAPROCESS/0D_SR.png', dst)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4lseWZpkk9L"
      },
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def createSR1(image_path):\n",
        "  IMAGE_ORIGINAL_PATH = image_path\n",
        "  IMAGE_SR_PATH = '/content/TensorFlow/DATAPROCESS/0D_SR.png'\n",
        "  PATH_TO_MODEL_DIR = '/content/MODEL_SR/FSRCNN_x2.pb'\n",
        "  #A LA IMAGEN ORIGINAL LE VOY A APLICAR UN AUMENTO DE X2\n",
        "  sr = dnn_superres.DnnSuperResImpl_create()\n",
        "  image = cv2.imread(IMAGE_ORIGINAL_PATH)\n",
        "  sr.readModel(PATH_TO_MODEL_DIR)\n",
        "  sr.setModel(\"fsrcnn\", 2)\n",
        "  result = sr.upsample(image)\n",
        "  cv2.imwrite(IMAGE_SR_PATH, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TzNHAOoLCKw"
      },
      "source": [
        "# ENFOQUE 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnjlpE2F2oIP"
      },
      "source": [
        "# **EVALUACIÓN MODIFICACIÓN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlp-7tBE24yX"
      },
      "source": [
        "import numpy as np\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "from numpy import sqrt\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.ticker as plticker\n",
        "import tensorflow as tf\n",
        "import statistics\n",
        "import time\n",
        "import cv2\n",
        "import glob\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "import shutil\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "def make_simple_inference240(image,counter,x1,y1,diccionario_final_predicciones,\n",
        "                                               diccionario_final_scores,\n",
        "                                               diccionario_final_clases,\n",
        "                                               width,height,etiquetas):\n",
        "  print('Running super-inference for {}... '.format(image), end='\\n')\n",
        "\n",
        "\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  input_tensor = tf.convert_to_tensor(image_np)\n",
        "  input_tensor = input_tensor[tf.newaxis, ...]\n",
        "  detections = detect_fn(input_tensor)\n",
        "  num_detections = int(detections.pop('num_detections'))\n",
        "  detections = {key: value[0, :num_detections].numpy()\n",
        "                   for key, value in detections.items()}\n",
        "\n",
        "  detections['num_detections'] = num_detections\n",
        "  detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "  image_np_with_detections = image_np.copy()\n",
        "\n",
        "  boxes_filter = []\n",
        "  scores_filter = []\n",
        "  classes_filter = []\n",
        "\n",
        "  for item in range(len(detections['detection_boxes'])):\n",
        "    if detections['detection_classes'][item] in etiquetas:\n",
        "      boxes_filter.append(detections['detection_boxes'][item].tolist())\n",
        "      scores_filter.append(detections['detection_scores'][item])\n",
        "      classes_filter.append(detections['detection_classes'][item])\n",
        "\n",
        "\n",
        "  detections['detection_boxes'] = np.array(boxes_filter)\n",
        "  detections['detection_classes'] = np.array(classes_filter)\n",
        "  detections['detection_scores'] = np.array(scores_filter)\n",
        "\n",
        "\n",
        "  boxes = detections['detection_boxes']\n",
        "  scores = detections['detection_scores'],\n",
        "  clases_detected = detections['detection_classes']\n",
        "  min_score_thresh = .4\n",
        "  true_boxes  = boxes[scores[0] > min_score_thresh]\n",
        "  true_scores = scores[0][scores[0] > min_score_thresh]\n",
        "  true_clases = clases_detected[scores[0] > min_score_thresh]\n",
        "\n",
        "\n",
        "  detections['detection_boxes'] = true_boxes\n",
        "  detections['detection_classes'] = true_clases\n",
        "  detections['detection_scores'] = true_scores\n",
        "  '''\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_detections,\n",
        "        detections['detection_boxes'],\n",
        "        detections['detection_classes'],\n",
        "        detections['detection_scores'],\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        max_boxes_to_draw=300,\n",
        "        min_score_thresh=.3,\n",
        "        agnostic_mode=False,\n",
        "        line_thickness=1)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image_np_with_detections)\n",
        "\n",
        "  min_score_thresh=.4\n",
        "  img = cv2.imread(image_path)\n",
        "  nombre = counter\n",
        "  plt.savefig('/content/fotoaux/'+str(nombre),  dpi=dpi ,bbox_inches='tight',pad_inches = 0)\n",
        "  plt.clf()\n",
        "  '''\n",
        "\n",
        "  for i in range(len(true_boxes)):\n",
        "      box_detected = true_boxes[i]\n",
        "      ymin = int(box_detected[0]*height)\n",
        "      xmin = int(box_detected[1]*width)\n",
        "      ymax = int(box_detected[2]*height)\n",
        "      xmax = int(box_detected[3]*width)\n",
        "      pointax = ((xmax)-(xmin))/2\n",
        "\n",
        "      ymin11 = ((((box_detected[0]*height))+y1)/2)\n",
        "      xmin11 = ((((box_detected[1]*width))+x1)/2)\n",
        "      ymax11 = ((((box_detected[2]*height))+y1)/2)\n",
        "      xmax11 = ((((box_detected[3]*width))+x1)/2)\n",
        "\n",
        "      coordenadas_good = []\n",
        "      coordenadas_good.append(ymin11/height)\n",
        "      coordenadas_good.append(xmin11/width)\n",
        "      coordenadas_good.append(ymax11/height)\n",
        "      coordenadas_good.append(xmax11/width)\n",
        "\n",
        "      diccionario_final_predicciones.append(coordenadas_good)\n",
        "      diccionario_final_scores.append(true_scores[i])\n",
        "      diccionario_final_clases.append(true_clases[i])\n",
        "\n",
        "\n",
        "def make_inference240(image_path,counter,image_save,etiquetas):\n",
        "  print('Running inference for {}... '.format(image_path), end='\\n')\n",
        "  createSR1(image_path)\n",
        "  image_np = load_image_into_numpy_array(image_path)\n",
        "  input_tensor = tf.convert_to_tensor(image_np)\n",
        "  input_tensor = input_tensor[tf.newaxis, ...]\n",
        "  detections = detect_fn(input_tensor)\n",
        "  num_detections = int(detections.pop('num_detections'))\n",
        "  detections = {key: value[0, :num_detections].numpy()\n",
        "                   for key, value in detections.items()}\n",
        "\n",
        "  detections['num_detections'] = num_detections\n",
        "  detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "  image_np_with_detections = image_np.copy()\n",
        "\n",
        "  min_score_thresh=.2\n",
        "  boxes = detections['detection_boxes']\n",
        "  scores = detections['detection_scores'],\n",
        "  clases_detected = detections['detection_classes']\n",
        "\n",
        "  true_boxes  = boxes[scores[0] > min_score_thresh]\n",
        "  true_scores = scores[0][scores[0] > min_score_thresh]\n",
        "  true_clases = clases_detected[scores[0] > min_score_thresh]\n",
        "\n",
        "  new_counter = 0\n",
        "  img = cv2.imread(image_path)\n",
        "  im = Image.open('/content/TensorFlow/DATAPROCESS/0D_SR.png')\n",
        "\n",
        "  contador_final = 0\n",
        "\n",
        "  img__READ = cv2.imread(image_path)\n",
        "  height, width, channels = img__READ.shape\n",
        "\n",
        "  diccionario_final_predicciones = []\n",
        "  diccionario_final_scores = []\n",
        "  diccionario_final_clases = []\n",
        "\n",
        "  for i in range(len(true_boxes)):\n",
        "      box_detected = true_boxes[i]\n",
        "      ymin = int(box_detected[0]*height*2)\n",
        "      xmin = int(box_detected[1]*width*2)\n",
        "      ymax = int(box_detected[2]*height*2)\n",
        "      xmax = int(box_detected[3]*width*2)\n",
        "\n",
        "      ymin11 = (box_detected[0])\n",
        "      xmin11 = (box_detected[1])\n",
        "      ymax11 = (box_detected[2])\n",
        "      xmax11 = (box_detected[3])\n",
        "\n",
        "      coordenadas_good = []\n",
        "      coordenadas_good.append(ymin11)\n",
        "      coordenadas_good.append(xmin11)\n",
        "      coordenadas_good.append(ymax11)\n",
        "      coordenadas_good.append(xmax11)\n",
        "\n",
        "      diccionario_final_predicciones.append(coordenadas_good)\n",
        "      diccionario_final_scores.append(true_scores[new_counter])\n",
        "      diccionario_final_clases.append(true_clases[new_counter])\n",
        "\n",
        "      a1 = (xmin+xmax)//2\n",
        "      a2 = (ymin+ymax)//2\n",
        "\n",
        "      RECORTEX = int(width/2)\n",
        "      RECORTEY = int(height/2)\n",
        "\n",
        "      '''izquierda arriba derecha abajo'''\n",
        "      im_crop_outside = im.crop((a1-RECORTEX, a2-RECORTEY, a1+RECORTEX, a2+RECORTEY))\n",
        "\n",
        "      nombre_sr_2 =\"/content/AUX/{}.jpg\".format(str(i))\n",
        "      im_crop_outside.save(nombre_sr_2, quality=100)\n",
        "\n",
        "      RECORTEX = int(width/2)\n",
        "      RECORTEY = int(height/2)\n",
        "\n",
        "      'INFERENCE'\n",
        "      score_class = make_simple_inference240(nombre_sr_2,new_counter,a1-RECORTEX,a2-RECORTEY,\n",
        "                                           diccionario_final_predicciones,\n",
        "                                           diccionario_final_scores,\n",
        "                                           diccionario_final_clases,\n",
        "                                           width,height,etiquetas )\n",
        "\n",
        "      new_counter = new_counter+1\n",
        "\n",
        "  detections['detection_boxes'] = np.array(diccionario_final_predicciones)\n",
        "  detections['detection_classes'] = diccionario_final_clases\n",
        "  detections['detection_scores'] = diccionario_final_scores\n",
        "\n",
        "\n",
        "  diccionario_final2_predicciones = []\n",
        "  diccionario_final2_scores = []\n",
        "  diccionario_final2_clases = []\n",
        "\n",
        "  puntero = 0\n",
        "  error = [-1,-1,-1,-1]\n",
        "\n",
        "  '''LLEGADOS A ESTE PUNTO TENGO QUE HACER IOU'''\n",
        "\n",
        "  for ia in range(len(diccionario_final_predicciones)):\n",
        "    coordenadas_A = diccionario_final_predicciones[ia]\n",
        "\n",
        "    if diccionario_final_predicciones[ia][0]!=-1:\n",
        "\n",
        "      diccionario_final2_predicciones.append(diccionario_final_predicciones[ia])\n",
        "      diccionario_final2_scores.append(diccionario_final_scores[ia])\n",
        "      diccionario_final2_clases.append(diccionario_final_clases[ia])\n",
        "\n",
        "\n",
        "      for ib in range(len(diccionario_final_predicciones)):\n",
        "        if ia != ib:\n",
        "\n",
        "          coordenadas_B = diccionario_final_predicciones[ib]\n",
        "\n",
        "          if diccionario_final_predicciones[ib][0]!=-1:\n",
        "              yminA = max(coordenadas_A[0]*height,coordenadas_B[0]*height)\n",
        "              xminA = max(coordenadas_A[1]*width,coordenadas_B[1]*width)\n",
        "              ymaxA = min(coordenadas_A[2]*height,coordenadas_B[2]*height)\n",
        "              xmaxA = min(coordenadas_A[3]*width,coordenadas_B[3]*width)\n",
        "\n",
        "              interArea = max(0, ymaxA - yminA + 1) * max(0, xmaxA - xminA + 1)\n",
        "\n",
        "              boxAArea = (coordenadas_A[2]*height - coordenadas_A[0]*height + 1) * (coordenadas_A[3]*width - coordenadas_A[1]*width + 1)\n",
        "              boxBArea = (coordenadas_B[2]*height - coordenadas_B[0]*height + 1) * (coordenadas_B[3]*width - coordenadas_B[1]*width + 1)\n",
        "\n",
        "              iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "              if iou > 0.1:\n",
        "                  contador = len(diccionario_final2_predicciones)-1\n",
        "                  if (diccionario_final2_scores[contador]) < diccionario_final_scores[ib]:\n",
        "\n",
        "                    contador = len(diccionario_final2_predicciones)-1\n",
        "                    diccionario_final2_predicciones[contador] = diccionario_final_predicciones[ib]\n",
        "                    diccionario_final2_scores[contador] = diccionario_final_scores[ib]\n",
        "                    diccionario_final2_clases[contador] = diccionario_final_clases[ib]\n",
        "\n",
        "                  diccionario_final_predicciones[ib] = error\n",
        "\n",
        "  detections['detection_boxes'] = np.array(diccionario_final2_predicciones)\n",
        "  detections['detection_classes'] = diccionario_final2_clases\n",
        "  detections['detection_scores'] = diccionario_final2_scores\n",
        "\n",
        "\n",
        "  boxes_filter = []\n",
        "  scores_filter = []\n",
        "  classes_filter = []\n",
        "\n",
        "  for item in range(len(detections['detection_boxes'])):\n",
        "    if detections['detection_classes'][item] in etiquetas:\n",
        "      boxes_filter.append(detections['detection_boxes'][item].tolist())\n",
        "      scores_filter.append(detections['detection_scores'][item])\n",
        "      classes_filter.append(detections['detection_classes'][item])\n",
        "\n",
        "\n",
        "  detections['detection_boxes'] = np.array(boxes_filter)\n",
        "  detections['detection_classes'] = np.array(classes_filter)\n",
        "  detections['detection_scores'] = np.array(scores_filter)\n",
        "\n",
        "\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_detections,\n",
        "        detections['detection_boxes'],\n",
        "        detections['detection_classes'],\n",
        "        detections['detection_scores'],\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        max_boxes_to_draw=300,\n",
        "        min_score_thresh=.3,\n",
        "        agnostic_mode=False,\n",
        "        line_thickness=1)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image_np_with_detections)\n",
        "\n",
        "  boxes = detections['detection_boxes']\n",
        "  scores = detections['detection_scores'],\n",
        "  clases_detected = detections['detection_classes']\n",
        "\n",
        "  min_score_thresh=.3\n",
        "  img = cv2.imread(image_path)\n",
        "  nombre = counter\n",
        "  plt.savefig('/content/RESULTADOS_SR/'+nombre,  dpi=dpi ,bbox_inches='tight',pad_inches = 0)\n",
        "  plt.clf()\n",
        "\n",
        "  boxes = detections['detection_boxes']\n",
        "  scores = detections['detection_scores'],\n",
        "  clases_detected = detections['detection_classes']\n",
        "\n",
        "  min_score_thresh = .3\n",
        "  salida = []\n",
        "  salida.append(boxes.tolist())\n",
        "  salida.append(scores[0])\n",
        "  salida.append(clases_detected)\n",
        "\n",
        "  image = Image.open(image_path)\n",
        "  width, height = image.size\n",
        "\n",
        "  salida.append(width)\n",
        "  salida.append(height)\n",
        "  return salida\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_DHzypY3pSI"
      },
      "source": [
        "# **CREACIÓN DEL JSON MODIFICADO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEG7gabed7mr"
      },
      "source": [
        "create_dir('/content/TensorFlow/DATAPROCESS')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANRO30ptiJBA"
      },
      "source": [
        "start_time = time()\n",
        "createSR1('/content/1.jpg')\n",
        "elapsed_time = time() - start_time\n",
        "print(elapsed_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_zXn0h708wg"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from time import time\n",
        "imagenes_dir2 = '/content/foto/'\n",
        "contenido2 = os.listdir(imagenes_dir2)\n",
        "imagenes_dir2 = []\n",
        "counter = 0\n",
        "result = []\n",
        "create_dir('/content/RESULTADOS_SR/')\n",
        "create_dir('/content/AUX/')\n",
        "\n",
        "tiempos_procesamiento2 = []\n",
        "\n",
        "import json\n",
        "\n",
        "out = []\n",
        "concon = 0\n",
        "for fichero2 in contenido2:\n",
        "  print(concon)\n",
        "  concon = concon +1\n",
        "  x = re.search(\"[1-9]+[0-9]*\", fichero2)\n",
        "  id = x.group(0)\n",
        "  image_path_gg = \"/content/foto/{}\".format(fichero2)\n",
        "  print(image_path_gg)\n",
        "  start_time = time()\n",
        "  createSR1(image_path_gg)\n",
        "  '''24'''\n",
        "  elapsed_time = time() - start_time\n",
        "  tiempos_procesamiento2.append(elapsed_time)\n",
        "  salida = make_inference240(image_path_gg,fichero2,'/content/RESULTADOS_SR/',[3,4,6,8])\n",
        "  print(\"\\n\")\n",
        "  counter = counter+1\n",
        "  result = []\n",
        "  converted_num = int(id)\n",
        "  converted_num = diccionario_ids[fichero2]\n",
        "  print(converted_num)\n",
        "\n",
        "  width = salida[3]\n",
        "  height = salida[4]\n",
        "\n",
        "  boxes2 = []\n",
        "\n",
        "\n",
        "  for box in salida[0]:\n",
        "    ymin = int(box[0]*height)\n",
        "    xmin = int(box[1]*width)\n",
        "    ymax = int(box[2]*height)\n",
        "    xmax = int(box[3]*width)\n",
        "\n",
        "\n",
        "    box_new = []\n",
        "    box_new.append(xmin)\n",
        "    box_new.append(ymin)\n",
        "    box_new.append(xmax-xmin)\n",
        "    box_new.append(ymax-ymin)\n",
        "\n",
        "\n",
        "    boxes2.append(box_new)\n",
        "\n",
        "  result.extend(\n",
        "        [\n",
        "            {\n",
        "                \"image_id\": converted_num,\n",
        "                \"category_id\": int(salida[2][k]),\n",
        "                \"bbox\": box,\n",
        "                \"score\": salida[1][k].astype(float),\n",
        "            }\n",
        "            for k, box in enumerate(boxes2)\n",
        "        ]\n",
        "  )\n",
        "\n",
        "  for sample in result:\n",
        "        out.append(sample)#detection result including\n",
        "\n",
        "with open('/content/test_data_modificado.json', 'w', encoding='utf-8') as file:\n",
        "  json.dump(out, file, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-aLjFtTxefR"
      },
      "source": [
        "print(tiempos_procesamiento2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MELwjeDy4dRj"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from time import time\n",
        "imagenes_dir2 = '/content/foto/'\n",
        "contenido2 = os.listdir(imagenes_dir2)\n",
        "imagenes_dir2 = []\n",
        "counter = 0\n",
        "result = []\n",
        "create_dir('/content/RESULTADOS_SR/')\n",
        "create_dir('/content/AUX/')\n",
        "\n",
        "tiempos_procesamiento2 = []\n",
        "\n",
        "import json\n",
        "\n",
        "out = []\n",
        "concon = 0\n",
        "for fichero2 in contenido2:\n",
        "  print(concon)\n",
        "  concon = concon +1\n",
        "  x = re.search(\"[1-9]+[0-9]*\", fichero2)\n",
        "  id = x.group(0)\n",
        "  image_path_gg = \"/content/foto/{}\".format(fichero2)\n",
        "  print(image_path_gg)\n",
        "  start_time = time()\n",
        "  createSR1(image_path_gg)\n",
        "  '''24'''\n",
        "  elapsed_time = time() - start_time\n",
        "  tiempos_procesamiento2.append(elapsed_time)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W-ACebbIdiz"
      },
      "source": [
        "# **Evaluación de las anotaciones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAaq0woyIgIK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import pylab\n",
        "import json\n",
        "\n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "\n",
        "\n",
        "annType = ['segm','bbox','keypoints']\n",
        "annType = annType[1]      #specify type here\n",
        "prefix = 'person_keypoints' if annType=='keypoints' else 'instances'\n",
        "print ('Running demo for *%s* results.'%(annType))\n",
        "\n",
        "# use the valadation labelme file\n",
        "# annFile = '/content/imagenesnormal.json'\n",
        "annFile = '/content/instances_default.json'\n",
        "cocoGt=COCO(annFile)\n",
        "#initialize COCO detections api\n",
        "# use the generated results\n",
        "resFile = '/content/test_data_modificado.json'\n",
        "cocoDt=cocoGt.loadRes(resFile)\n",
        "\n",
        "\n",
        "dts = json.load(open(resFile,'r'))\n",
        "imgIds = [imid['image_id'] for imid in dts]\n",
        "imgIds = sorted(list(set(imgIds)))\n",
        "\n",
        "\n",
        "'''\n",
        "imgIds=sorted(cocoGt.getImgIds())\n",
        "imgIds=imgIds[0:24]\n",
        "imgId = imgIds[np.random.randint(24)]\n",
        "'''\n",
        "\n",
        "# running box evaluation\n",
        "cocoEval = COCOeval(cocoGt,cocoDt,annType)\n",
        "\n",
        "\n",
        "cocoEval.params.imgIds  = imgIds\n",
        "\n",
        "cocoEval.params.catIds = [3] # 1 stands for the 'person' class, you can increase or decrease the category as needed\n",
        "\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdfI_LQf8C5V"
      },
      "source": [
        "!zip -r '/content/RESULTADOS_SR.zip' '/content/RESULTADOS_SR'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlXvKC44SGBm"
      },
      "source": [
        "!zip -r '/content/AUX.zip' '/content/AUX'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkLjSxXxXDDe"
      },
      "source": [
        "create_dir('/content/AUX')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fsvlEtUSF_Q"
      },
      "source": [
        "make_inference240('/content/foto/10.jpg',fichero2,'/content/foto/10.jpg',[3,4,6,8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BXVa6umugqa"
      },
      "source": [
        "# **DIBUJAR LOS GT DE LAS ANOTACIONES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvoBxIa_3mYz"
      },
      "source": [
        "def create_image(image_path,counter):\n",
        "  imga = cv2.imread(image_path)\n",
        "  nombrenuevo = '/content/valGT/'+counter\n",
        "  cv2.imwrite(nombrenuevo, imga)\n",
        "  return nombrenuevo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmIhamUQebk7"
      },
      "source": [
        "def draw_rectangle(image_path,x1,y1,x2,y2):\n",
        "  x1 = round(x1)\n",
        "  y1 = round(y1)\n",
        "  x2 = round(x2)\n",
        "  y2 = round(y2)\n",
        "\n",
        "  start_point = (x1, y1)\n",
        "  end_point = (x1+x2, y1+y2)\n",
        "  color = (255, 0, 0)\n",
        "  thickness = 2\n",
        "\n",
        "  imga = cv2.imread(image_path)\n",
        "  image = cv2.rectangle(imga, start_point, end_point, color, thickness)\n",
        "  cv2.imwrite(image_path, imga)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMvOMQATSnKo"
      },
      "source": [
        "import json\n",
        "import shutil\n",
        "import cv2\n",
        "\n",
        "jsonString = '/content/instances_default.json'\n",
        "images_dir_dataprocess = '/content/valGT/'\n",
        "create_dir(images_dir_dataprocess)\n",
        "\n",
        "contenido2 = os.listdir('/content/foto')\n",
        "contador = 0\n",
        "for fichero in contenido2:\n",
        "  nombre_good = '/content/foto/'+fichero\n",
        "  id_frame = diccionario_ids[fichero]\n",
        "  salida = create_image(nombre_good,fichero)\n",
        "  contador=contador+1\n",
        "  with open(jsonString) as json_file:\n",
        "      data = json.load(json_file)\n",
        "      for p3 in data['annotations']:\n",
        "          nombre_identificador = p3['image_id']\n",
        "          if nombre_identificador == id_frame:\n",
        "            lista = p3['bbox']\n",
        "            draw_rectangle(salida,lista[0],lista[1],lista[2],lista[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmEwYEaBkGkc"
      },
      "source": [
        "!zip -r '/content/valGT/RESULTADOS_GT.zip' '/content/valGT'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTUqKMBd4jyC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_t2Scg14kWQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHVxDyru4koB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQouZI2iO4QE"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ela8Oym5QYUL"
      },
      "source": [
        "# **CLASE 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "gTMKD_HGbv9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "W_N1FVD4byph"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbcLZ02Cgsnk"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/instances_default.json'\n",
        "diccionario_ids = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data['images']:\n",
        "        data_s_image = p3['file_name']\n",
        "        data_s_id = p3['id']\n",
        "        diccionario_ids[data_s_image] = data_s_id\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD8cvCGaCtYi"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/instances_default.json'\n",
        "diccionario_ids2 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data['annotations']:\n",
        "        image_id_frame = p3['image_id']\n",
        "        clase = p3['category_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        if clase==3:\n",
        "          if image_name_frame in diccionario_ids2:\n",
        "            diccionario_ids2[image_name_frame] = diccionario_ids2[image_name_frame]+1\n",
        "          elif image_name_frame not in diccionario_ids2:\n",
        "            diccionario_ids2[image_name_frame] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaTcNbxrG-Yf"
      },
      "source": [
        "valores_ord2 = dict(sorted(diccionario_ids2.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRC1e2LYFPJ8"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/test_data_modificado.json'\n",
        "diccionario_ids3 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data:\n",
        "        image_id_frame = p3['image_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        clase = p3['category_id']\n",
        "        if image_name_frame in diccionario_ids3 and clase == 3:\n",
        "          diccionario_ids3[image_name_frame] = diccionario_ids3[image_name_frame]+1\n",
        "        elif image_name_frame not in diccionario_ids3 and clase == 3:\n",
        "          diccionario_ids3[image_name_frame] = 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUWnszSOJM6O"
      },
      "source": [
        "lista_modificada = [k  for  k in  diccionario_ids2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dTfpq3IJqR0"
      },
      "source": [
        "lista_modificada2 = natsorted(lista_modificada)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgkJb2jOqsdk"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/test_data_normal.json'\n",
        "diccionario_ids4 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data:\n",
        "        image_id_frame = p3['image_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        clase = p3['category_id']\n",
        "        if image_name_frame in diccionario_ids4 and clase == 3:\n",
        "          diccionario_ids4[image_name_frame] = diccionario_ids4[image_name_frame]+1\n",
        "        elif image_name_frame not in diccionario_ids4 and clase == 3:\n",
        "          diccionario_ids4[image_name_frame] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa2LaEp3KXG8"
      },
      "source": [
        "for item in lista_modificada2:\n",
        "  valor_1 = 0\n",
        "  if item not in diccionario_ids3:\n",
        "    valor_1 = 0\n",
        "  else:\n",
        "    valor_1 = diccionario_ids3[item]\n",
        "\n",
        "  if item not in diccionario_ids4:\n",
        "    valor_2 = 0\n",
        "  else:\n",
        "    valor_2 = diccionario_ids4[item]\n",
        "\n",
        "\n",
        "  print(item,\",\",diccionario_ids2[item],\",\",valor_1,\",\",valor_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZRv-n2NQldV"
      },
      "source": [
        "# **CLASE 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QJHuNKKQnmD"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/instances_default.json'\n",
        "diccionario_ids = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data['images']:\n",
        "        data_s_image = p3['file_name']\n",
        "        data_s_id = p3['id']\n",
        "        diccionario_ids[data_s_image] = data_s_id\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS8ZD1nVQnmE"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/instances_default.json'\n",
        "diccionario_ids2 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data['annotations']:\n",
        "        image_id_frame = p3['image_id']\n",
        "        clase = p3['category_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        if clase==4:\n",
        "          if image_name_frame in diccionario_ids2:\n",
        "            diccionario_ids2[image_name_frame] = diccionario_ids2[image_name_frame]+1\n",
        "          elif image_name_frame not in diccionario_ids2:\n",
        "            diccionario_ids2[image_name_frame] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_DuZtcyQnmE"
      },
      "source": [
        "valores_ord2 = dict(sorted(diccionario_ids2.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXTaBJIHQnmE"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/test_data_modificado.json'\n",
        "diccionario_ids3 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data:\n",
        "        image_id_frame = p3['image_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        clase = p3['category_id']\n",
        "        if image_name_frame in diccionario_ids3 and clase == 4:\n",
        "          diccionario_ids3[image_name_frame] = diccionario_ids3[image_name_frame]+1\n",
        "        elif image_name_frame not in diccionario_ids3 and clase == 4:\n",
        "          diccionario_ids3[image_name_frame] = 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL9tXuYBQnmF"
      },
      "source": [
        "lista_modificada = [k  for  k in  diccionario_ids2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UTvUpACQnmF"
      },
      "source": [
        "lista_modificada2 = natsorted(lista_modificada)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL27R_2RQnmF"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/test_data_normal.json'\n",
        "diccionario_ids4 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data:\n",
        "        image_id_frame = p3['image_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        clase = p3['category_id']\n",
        "        if image_name_frame in diccionario_ids4 and clase == 4:\n",
        "          diccionario_ids4[image_name_frame] = diccionario_ids4[image_name_frame]+1\n",
        "        elif image_name_frame not in diccionario_ids4 and clase == 4:\n",
        "          diccionario_ids4[image_name_frame] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVwW-Sw2QnmF"
      },
      "source": [
        "for item in lista_modificada2:\n",
        "  valor_1 = 0\n",
        "  if item not in diccionario_ids3:\n",
        "    valor_1 = 0\n",
        "  else:\n",
        "    valor_1 = diccionario_ids3[item]\n",
        "\n",
        "  if item not in diccionario_ids4:\n",
        "    valor_2 = 0\n",
        "  else:\n",
        "    valor_2 = diccionario_ids4[item]\n",
        "\n",
        "\n",
        "  print(item,\",\",diccionario_ids2[item],\",\",valor_1,\",\",valor_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLTNhJ5ZQo6X"
      },
      "source": [
        "# **CLASE 6**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpLf1teVQrqH"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/instances_default.json'\n",
        "diccionario_ids = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data['images']:\n",
        "        data_s_image = p3['file_name']\n",
        "        data_s_id = p3['id']\n",
        "        diccionario_ids[data_s_image] = data_s_id\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxLOVy5qQrqI"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/instances_default.json'\n",
        "diccionario_ids2 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data['annotations']:\n",
        "        image_id_frame = p3['image_id']\n",
        "        clase = p3['category_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        if clase==6:\n",
        "          if image_name_frame in diccionario_ids2:\n",
        "            diccionario_ids2[image_name_frame] = diccionario_ids2[image_name_frame]+1\n",
        "          elif image_name_frame not in diccionario_ids2:\n",
        "            diccionario_ids2[image_name_frame] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cdxMRSFQrqI"
      },
      "source": [
        "valores_ord2 = dict(sorted(diccionario_ids2.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-KbAZhEQrqI"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/test_data_modificado.json'\n",
        "diccionario_ids3 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data:\n",
        "        image_id_frame = p3['image_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        clase = p3['category_id']\n",
        "        if image_name_frame in diccionario_ids3 and clase == 6:\n",
        "          diccionario_ids3[image_name_frame] = diccionario_ids3[image_name_frame]+1\n",
        "        elif image_name_frame not in diccionario_ids3 and clase == 6:\n",
        "          diccionario_ids3[image_name_frame] = 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAf7QhCRQrqI"
      },
      "source": [
        "lista_modificada = [k  for  k in  diccionario_ids2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpwOkQVSQrqJ"
      },
      "source": [
        "lista_modificada2 = natsorted(lista_modificada)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c8SB6E5QrqJ"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/test_data_normal.json'\n",
        "diccionario_ids4 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data:\n",
        "        image_id_frame = p3['image_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        clase = p3['category_id']\n",
        "        if image_name_frame in diccionario_ids4 and clase == 6:\n",
        "          diccionario_ids4[image_name_frame] = diccionario_ids4[image_name_frame]+1\n",
        "        elif image_name_frame not in diccionario_ids4 and clase == 6:\n",
        "          diccionario_ids4[image_name_frame] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KskmuVsrQrqJ"
      },
      "source": [
        "for item in lista_modificada2:\n",
        "  valor_1 = 0\n",
        "  if item not in diccionario_ids3:\n",
        "    valor_1 = 0\n",
        "  else:\n",
        "    valor_1 = diccionario_ids3[item]\n",
        "\n",
        "  if item not in diccionario_ids4:\n",
        "    valor_2 = 0\n",
        "  else:\n",
        "    valor_2 = diccionario_ids4[item]\n",
        "\n",
        "\n",
        "  print(item,\",\",diccionario_ids2[item],\",\",valor_1,\",\",valor_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhxPhVgbQthH"
      },
      "source": [
        "# **CLASE 8**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8gcm8POQuzF"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/instances_default.json'\n",
        "diccionario_ids = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data['images']:\n",
        "        data_s_image = p3['file_name']\n",
        "        data_s_id = p3['id']\n",
        "        diccionario_ids[data_s_image] = data_s_id\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17wUeAalQuzG"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/instances_default.json'\n",
        "diccionario_ids2 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data['annotations']:\n",
        "        image_id_frame = p3['image_id']\n",
        "        clase = p3['category_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        if clase==8:\n",
        "          if image_name_frame in diccionario_ids2:\n",
        "            diccionario_ids2[image_name_frame] = diccionario_ids2[image_name_frame]+1\n",
        "          elif image_name_frame not in diccionario_ids2:\n",
        "            diccionario_ids2[image_name_frame] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y96VnWJPQuzG"
      },
      "source": [
        "valores_ord2 = dict(sorted(diccionario_ids2.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h83SOpewQuzG"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/test_data_modificado.json'\n",
        "diccionario_ids3 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data:\n",
        "        image_id_frame = p3['image_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        clase = p3['category_id']\n",
        "        if image_name_frame in diccionario_ids3 and clase == 8:\n",
        "          diccionario_ids3[image_name_frame] = diccionario_ids3[image_name_frame]+1\n",
        "        elif image_name_frame not in diccionario_ids3 and clase == 8:\n",
        "          diccionario_ids3[image_name_frame] = 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1gsLrkSQuzG"
      },
      "source": [
        "lista_modificada = [k  for  k in  diccionario_ids2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYwbhK77QuzH"
      },
      "source": [
        "lista_modificada2 = natsorted(lista_modificada)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73D7pJzVQuzH"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "jsonString = '/content/test_data_normal.json'\n",
        "diccionario_ids4 = {}\n",
        "\n",
        "with open(jsonString) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    for p3 in data:\n",
        "        image_id_frame = p3['image_id']\n",
        "        image_name_frame = ''\n",
        "        for frame, index in diccionario_ids.items():\n",
        "          if index == image_id_frame:\n",
        "            image_name_frame = frame\n",
        "        clase = p3['category_id']\n",
        "        if image_name_frame in diccionario_ids4 and clase == 8:\n",
        "          diccionario_ids4[image_name_frame] = diccionario_ids4[image_name_frame]+1\n",
        "        elif image_name_frame not in diccionario_ids4 and clase == 8:\n",
        "          diccionario_ids4[image_name_frame] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5ZUvmtoQuzH"
      },
      "source": [
        "for item in lista_modificada2:\n",
        "  valor_1 = 0\n",
        "  if item not in diccionario_ids3:\n",
        "    valor_1 = 0\n",
        "  else:\n",
        "    valor_1 = diccionario_ids3[item]\n",
        "\n",
        "  if item not in diccionario_ids4:\n",
        "    valor_2 = 0\n",
        "  else:\n",
        "    valor_2 = diccionario_ids4[item]\n",
        "\n",
        "\n",
        "  print(item,\",\",diccionario_ids2[item],\",\",valor_1,\",\",valor_2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s60D0xzk8KoH"
      },
      "source": [
        "# **CREACIÓN DEL JSON TO XML**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjpQN2_f8KSS"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import numpy as np\n",
        "from time import time\n",
        "from dict2xml import dict2xml\n",
        "import json\n",
        "\n",
        "imagenes_dir2 = '/content/TensorFlow/workspace/training_demo/images/imagenes/'\n",
        "contenido2 = os.listdir(imagenes_dir2)\n",
        "imagenes_dir2 = []\n",
        "counter = 0\n",
        "result = []\n",
        "create_dir('/content/RESULTADOS_SR/')\n",
        "create_dir('/content/AUX/')\n",
        "\n",
        "tiempos_procesamiento2 = []\n",
        "\n",
        "import json\n",
        "\n",
        "out = []\n",
        "concon = 0\n",
        "\n",
        "for fichero2 in contenido2:\n",
        "  concon = concon +1\n",
        "  x = re.search(\"[1-9]+[0-9]*\", fichero2)\n",
        "  id = x.group(0)\n",
        "  image_path_gg = \"/content/TensorFlow/workspace/training_demo/images/imagenes/{}\".format(fichero2)\n",
        "  image_path_gg2 = \"/content/TensorFlow/workspace/training_demo/images/train/{}\".format(fichero2)\n",
        "  print(image_path_gg)\n",
        "  start_time = time()\n",
        "  createSR1(image_path_gg)\n",
        "  '''24'''\n",
        "  elapsed_time = time() - start_time\n",
        "  tiempos_procesamiento2.append(elapsed_time)\n",
        "  salida = make_inference240(image_path_gg,fichero2,'/content/RESULTADOS_SR/',[3])\n",
        "  print(\"\\n\")\n",
        "  counter = counter+1\n",
        "  result = []\n",
        "\n",
        "  width = salida[3]\n",
        "  height = salida[4]\n",
        "\n",
        "\n",
        "  x = '{\"annotation\":{\"folder\": \"images\",\"filename\": \"1.jpg\",\"path\": \"/Users/ivan/Desktop/images/1.jpg\",\"source\": {\"database\": \"Unknown\"},\"size\": {\"width\": \"640\",\"height\": \"480\",\"depth\": \"3\"},\"segmented\": \"0\",\"object\": []}}'\n",
        "  y = json.loads(x)\n",
        "  y[\"annotation\"][\"folder\"] = 'imagenes'\n",
        "  y[\"annotation\"][\"filename\"] = fichero2\n",
        "  y[\"annotation\"][\"path\"] = image_path_gg\n",
        "\n",
        "  y[\"annotation\"][\"size\"][\"width\"] = width\n",
        "  y[\"annotation\"][\"size\"][\"height\"] = height\n",
        "\n",
        "  shutil.copyfile(image_path_gg, image_path_gg2)\n",
        "\n",
        "\n",
        "  boxes2 = []\n",
        "\n",
        "\n",
        "  for box in salida[0]:\n",
        "    ymin = int(box[0]*height)\n",
        "    xmin = int(box[1]*width)\n",
        "    ymax = int(box[2]*height)\n",
        "    xmax = int(box[3]*width)\n",
        "\n",
        "\n",
        "    objeto1 = '{\"name\": \"car\",\"pose\": \"Unspecified\",\"truncated\": \"0\",\"difficult\": \"0\",\"bndbox\": {\"xmin\": \"302\",\"ymin\": \"215\",\"xmax\": \"338\",\"ymax\": \"237\"}}'\n",
        "    z = json.loads(objeto1)\n",
        "    z[\"name\"] = \"car\"\n",
        "    z[\"bndbox\"][\"xmin\"] = xmin\n",
        "    z[\"bndbox\"][\"xmax\"] = xmax\n",
        "    z[\"bndbox\"][\"ymin\"] = ymin\n",
        "    z[\"bndbox\"][\"ymax\"] = ymax\n",
        "\n",
        "    y['annotation']['object'].append(z)\n",
        "\n",
        "  nombre_bueno = '/content/TensorFlow/workspace/training_demo/images/imagenes_json/'+id+'.json'\n",
        "  nombre_bueno2 = '/content/TensorFlow/workspace/training_demo/images/train/'+id+'.xml'\n",
        "  print(y)\n",
        "  with open(nombre_bueno, 'w', encoding='utf-8') as f:\n",
        "    json.dump(y, f)\n",
        "\n",
        "  '''hacerlo en xml'''\n",
        "  with open(nombre_bueno) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    xml = dict2xml(data)\n",
        "    with open(nombre_bueno2, \"w\") as f:\n",
        "      f.write(xml)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4fSukMkV9Mz"
      },
      "source": [
        "import collections.abc\n",
        "import collections\n",
        "import re\n",
        "\n",
        "start_ranges = \"|\".join(\n",
        "    \"[{0}]\".format(r)\n",
        "    for r in [\n",
        "        \"\\xC0-\\xD6\",\n",
        "        \"\\xD8-\\xF6\",\n",
        "        \"\\xF8-\\u02FF\",\n",
        "        \"\\u0370-\\u037D\",\n",
        "        \"\\u037F-\\u1FFF\",\n",
        "        \"\\u200C-\\u200D\",\n",
        "        \"\\u2070-\\u218F\",\n",
        "        \"\\u2C00-\\u2FEF\",\n",
        "        \"\\u3001-\\uD7FF\",\n",
        "        \"\\uF900-\\uFDCF\",\n",
        "        \"\\uFDF0-\\uFFFD\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "NameStartChar = re.compile(r\"(:|[A-Z]|_|[a-z]|{0})\".format(start_ranges))\n",
        "NameChar = re.compile(r\"(\\-|\\.|[0-9]|\\xB7|[\\u0300-\\u036F]|[\\u203F-\\u2040])\")\n",
        "\n",
        "########################\n",
        "###   NODE\n",
        "########################\n",
        "\n",
        "\n",
        "class Node(object):\n",
        "    \"\"\"\n",
        "        Represents each tag in the tree\n",
        "\n",
        "        Each node has _either_ a single value or one or more children\n",
        "        If it has a value:\n",
        "            The serialized result is <%(tag)s>%(value)s</%(tag)s>\n",
        "\n",
        "        If it has children:\n",
        "            The serialized result is\n",
        "                <%(wrap)s>\n",
        "                    %(children)s\n",
        "                </%(wrap)s>\n",
        "\n",
        "        Which one it is depends on the implementation of self.convert\n",
        "    \"\"\"\n",
        "\n",
        "    # A mapping of characters to treat as escapable entities and their replacements\n",
        "    entities = [(\"&\", \"&amp;\"), (\"<\", \"&lt;\"), (\">\", \"&gt;\")]\n",
        "\n",
        "    def __init__(self, wrap=\"\", tag=\"\", data=None, iterables_repeat_wrap=True):\n",
        "        self.tag = self.sanitize_element(tag)\n",
        "        self.wrap = self.sanitize_element(wrap)\n",
        "        self.data = data\n",
        "        self.type = self.determine_type()\n",
        "        self.iterables_repeat_wrap = iterables_repeat_wrap\n",
        "\n",
        "        if self.type == \"flat\" and isinstance(self.data, str):\n",
        "            # Make sure we deal with entities\n",
        "            for entity, replacement in self.entities:\n",
        "                self.data = self.data.replace(entity, replacement)\n",
        "\n",
        "    def serialize(self, indenter):\n",
        "        \"\"\"Returns the Node serialized as an xml string\"\"\"\n",
        "        # Determine the start and end of this node\n",
        "        wrap = self.wrap\n",
        "        end, start = \"\", \"\"\n",
        "        if wrap:\n",
        "            end = \"</{0}>\".format(wrap)\n",
        "            start = \"<{0}>\".format(wrap)\n",
        "\n",
        "        # Convert the data attached in this node into a value and children\n",
        "        value, children = self.convert()\n",
        "\n",
        "        # Determine the content of the node (essentially the children as a string value)\n",
        "        content = \"\"\n",
        "        if children:\n",
        "            if self.type != \"iterable\":\n",
        "                # Non-iterable wraps all it's children in the same tag\n",
        "                content = indenter((c.serialize(indenter) for c in children), wrap)\n",
        "            else:\n",
        "                if self.iterables_repeat_wrap:\n",
        "                    # Iterables repeat the wrap for each child\n",
        "                    result = []\n",
        "                    for c in children:\n",
        "                        content = c.serialize(indenter)\n",
        "                        if c.type == \"flat\":\n",
        "                            # Child with value, it already is surrounded by the tag\n",
        "                            result.append(content)\n",
        "                        else:\n",
        "                            # Child with children of it's own, they need to be wrapped by start and end\n",
        "                            content = indenter([content], True)\n",
        "                            result.append(\"\".join((start, content, end)))\n",
        "\n",
        "                    # We already have what we want, return the indented result\n",
        "                    return indenter(result, False)\n",
        "                else:\n",
        "                    result = []\n",
        "                    for c in children:\n",
        "                        result.append(c.serialize(indenter))\n",
        "                    return \"\".join([start, indenter(result, True), end])\n",
        "\n",
        "        # If here, either:\n",
        "        #  * Have a value\n",
        "        #  * Or this node is not an iterable\n",
        "        return \"\".join((start, value, content, end))\n",
        "\n",
        "    def determine_type(self):\n",
        "        \"\"\"\n",
        "            Return the type of the data on this node as an identifying string\n",
        "\n",
        "            * Iterable : Supports \"for item in data\"\n",
        "            * Mapping : Supports \"for key in data: value = data[key]\"\n",
        "            * flat : A string or something that isn't iterable or a mapping\n",
        "        \"\"\"\n",
        "        data = self.data\n",
        "        if isinstance(data, str):\n",
        "            return \"flat\"\n",
        "        elif isinstance(data, collections.abc.Mapping):\n",
        "            return \"mapping\"\n",
        "        elif isinstance(data, collections.abc.Iterable):\n",
        "            return \"iterable\"\n",
        "        else:\n",
        "            return \"flat\"\n",
        "\n",
        "    def convert(self):\n",
        "        \"\"\"\n",
        "            Convert data on this node into a (value, children) tuple depending on the type of the data\n",
        "            If the type is :\n",
        "                * flat : Use self.tag to surround the value. <tag>value</tag>\n",
        "                * mapping : Return a list of tags where the key for each child is the wrap for that node\n",
        "                * iterable : Return a list of Nodes where self.wrap is the tag for that node\n",
        "        \"\"\"\n",
        "        val = \"\"\n",
        "        typ = self.type\n",
        "        data = self.data\n",
        "        children = []\n",
        "\n",
        "        if typ == \"mapping\":\n",
        "            sorted_data = data\n",
        "            if not isinstance(data, collections.OrderedDict):\n",
        "                sorted_data = (data)\n",
        "\n",
        "            for key in sorted_data:\n",
        "                item = data[key]\n",
        "                children.append(\n",
        "                    Node(key, \"\", item, iterables_repeat_wrap=self.iterables_repeat_wrap)\n",
        "                )\n",
        "\n",
        "        elif typ == \"iterable\":\n",
        "            for item in data:\n",
        "                children.append(\n",
        "                    Node(\"\", self.wrap, item, iterables_repeat_wrap=self.iterables_repeat_wrap,)\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            val = str(data)\n",
        "            if self.tag:\n",
        "                val = \"<{0}>{1}</{2}>\".format(self.tag, val, self.tag)\n",
        "\n",
        "        return val, children\n",
        "\n",
        "    @staticmethod\n",
        "    def sanitize_element(wrap):\n",
        "        \"\"\"\n",
        "            Convert `wrap` into a valid tag name applying the XML Naming Rules.\n",
        "\n",
        "                * Names can contain letters, numbers, and other characters\n",
        "                * Names cannot start with a number or punctuation character\n",
        "                * Names cannot start with the letters xml (or XML, or Xml, etc)\n",
        "                * Names cannot contain spaces\n",
        "                * Any name can be used, no words are reserved.\n",
        "\n",
        "            :ref: http://www.w3.org/TR/REC-xml/#NT-NameChar\n",
        "        \"\"\"\n",
        "        if wrap and isinstance(wrap, str):\n",
        "            if wrap.lower().startswith(\"xml\"):\n",
        "                wrap = \"_\" + wrap\n",
        "            return \"\".join(\n",
        "                [\"_\" if not NameStartChar.match(wrap) else \"\"]\n",
        "                + [\"_\" if not (NameStartChar.match(c) or NameChar.match(c)) else c for c in wrap]\n",
        "            )\n",
        "        else:\n",
        "            return wrap\n",
        "\n",
        "\n",
        "########################\n",
        "###   CONVERTER\n",
        "########################\n",
        "\n",
        "\n",
        "class Converter(object):\n",
        "    \"\"\"Logic for creating a Node tree and serialising that tree into a string\"\"\"\n",
        "\n",
        "    def __init__(self, wrap=None, indent=\"  \", newlines=True):\n",
        "        \"\"\"\n",
        "            wrap: The tag that the everything else will be contained within\n",
        "            indent: The string that is multiplied at the start of each new line, to represent each level of nesting\n",
        "            newlines: A boolean specifying whether we want each tag on a new line.\n",
        "\n",
        "            Note that indent only works if newlines is True\n",
        "        \"\"\"\n",
        "        self.wrap = wrap\n",
        "        self.indent = indent\n",
        "        self.newlines = newlines\n",
        "\n",
        "    def _make_indenter(self):\n",
        "        \"\"\"Returns a function that given a list of strings, will return that list as a single, indented, string\"\"\"\n",
        "        indent = self.indent\n",
        "        newlines = self.newlines\n",
        "\n",
        "        if not newlines:\n",
        "            # No newlines, don't care about indentation\n",
        "            ret = lambda nodes, wrapped: \"\".join(nodes)\n",
        "        else:\n",
        "            if not indent:\n",
        "                indent = \"\"\n",
        "\n",
        "            def eachline(nodes):\n",
        "                \"\"\"Yield each line in each node\"\"\"\n",
        "                for node in nodes:\n",
        "                    for line in node.split(\"\\n\"):\n",
        "                        yield line\n",
        "\n",
        "            def ret(nodes, wrapped):\n",
        "                \"\"\"\n",
        "                    Indent nodes depending on value of wrapped and indent\n",
        "                    If not wrapped, then don't indent\n",
        "                    Otherwise,\n",
        "                        Seperate each child by a newline\n",
        "                        and indent each line in the child by one indent unit\n",
        "                \"\"\"\n",
        "                if wrapped:\n",
        "                    seperator = \"\\n{0}\".format(indent)\n",
        "                    surrounding = \"\\n{0}{{0}}\\n\".format(indent)\n",
        "                else:\n",
        "                    seperator = \"\\n\"\n",
        "                    surrounding = \"{0}\"\n",
        "                return surrounding.format(seperator.join(eachline(nodes)))\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def build(self, data, iterables_repeat_wrap=True):\n",
        "        \"\"\"Create a Node tree from the data and return it as a serialized xml string\"\"\"\n",
        "        indenter = self._make_indenter()\n",
        "        return Node(\n",
        "            wrap=self.wrap, data=data, iterables_repeat_wrap=iterables_repeat_wrap\n",
        "        ).serialize(indenter)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuDQjFRqS86R"
      },
      "source": [
        "def dict2xml(data, *args, **kwargs):\n",
        "    \"\"\"Return an XML string of a Python dict object.\"\"\"\n",
        "    return Converter(*args, **kwargs).build(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kHQ8kJHTVAt"
      },
      "source": [
        "import json\n",
        "with open('/content/1.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    xml = dict2xml(data)\n",
        "    print(xml)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S80-Zvz1Ivp9"
      },
      "source": [
        "# **XML TO TFRECORDS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zJlJgK0TXuR"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gANOfRTUJX--"
      },
      "source": [
        "print(PATH_TO_LABELS)\n",
        "shutil.copyfile(PATH_TO_LABELS, '/content/record/label_map.pbtxt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1atelBf0K1m"
      },
      "source": [
        "import time\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "'''\n",
        "PATH_TO_CFG = \"/content/drive/MyDrive/EfficientMod/ssd_efficientdet_d4_1024x1024_coco17_tpu-32.config\"\n",
        "PATH_TO_CKPT = \"/content/drive/MyDrive/EfficientMod\"\n",
        "\n",
        "\n",
        "PATH_TO_CFG = \"/content/ppt/efficientdet_d4_coco17_tpu-32/pipeline.config\"\n",
        "PATH_TO_CKPT = \"/content/ppt/efficientdet_d4_coco17_tpu-32/checkpoint\"\n",
        "\n",
        "PATH_TO_CFG = \"/content/ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8.config\"\n",
        "PATH_TO_CKPT = \"/content/FASTERFT/checkpoint\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "PATH_TO_CFG = \"/content/NECESARIO/faster_rcnn_resnet152_v1_1024x1024_coco17_tpu-8.config\"\n",
        "PATH_TO_CKPT = \"/content/FASTERFT/checkpoint\"\n",
        "\n",
        "'''\n",
        "\n",
        "PATH_TO_CFG = \"/content/NECESARIO/ssd_efficientdet_d4_1024x1024_coco17_tpu-32.config\"\n",
        "PATH_TO_CKPT = \"/content/mimodelocompilado/checkpoint\"\n",
        "\n",
        "print('Loading model... ', end='')\n",
        "start_time = time.time()\n",
        "\n",
        "# Load pipeline config and build a detection model\n",
        "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\n",
        "model_config = configs['model']\n",
        "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
        "\n",
        "# Restore checkpoint\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()\n",
        "\n",
        "@tf.function\n",
        "def detect_fn(image):\n",
        "    \"\"\"Detect objects in image.\"\"\"\n",
        "\n",
        "    image, shapes = detection_model.preprocess(image)\n",
        "    prediction_dict = detection_model.predict(image, shapes)\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "    return detections\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print('Done! Took {} seconds'.format(elapsed_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kb4tx5V0UdJ"
      },
      "source": [
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
        "                                                                    use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2Wp4BRF0ckD"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "\n",
        "def make_checkpoint45(image_path,counter,image_save,etiquetas):\n",
        "\n",
        "    image_np = load_image_into_numpy_array(image_path)\n",
        "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "    detections = detect_fn(input_tensor)\n",
        "    num_detections = int(detections.pop('num_detections'))\n",
        "    detections = {key: value[0, :num_detections].numpy()\n",
        "                  for key, value in detections.items()}\n",
        "    detections['num_detections'] = num_detections\n",
        "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "    image_np_with_detections = image_np.copy()\n",
        "\n",
        "    boxes_filter = []\n",
        "    scores_filter = []\n",
        "    classes_filter = []\n",
        "\n",
        "    for item in range(len(detections['detection_boxes'])):\n",
        "      if detections['detection_classes'][item] in etiquetas:\n",
        "        boxes_filter.append(detections['detection_boxes'][item].tolist())\n",
        "        scores_filter.append(detections['detection_scores'][item])\n",
        "        classes_filter.append(detections['detection_classes'][item])\n",
        "\n",
        "\n",
        "    detections['detection_boxes'] = np.array(boxes_filter)\n",
        "    detections['detection_classes'] = np.array(classes_filter)\n",
        "    detections['detection_scores'] = np.array(scores_filter)\n",
        "\n",
        "    label_id_offset = 1\n",
        "\n",
        "\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "            image_np_with_detections,\n",
        "            detections['detection_boxes'],\n",
        "            detections['detection_classes']+label_id_offset,\n",
        "            detections['detection_scores'],\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            max_boxes_to_draw=200,\n",
        "            min_score_thresh=.3,\n",
        "            agnostic_mode=False)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "    plt.imshow(image_np_with_detections)\n",
        "    nombre = counter\n",
        "    plt.savefig(image_save+nombre,  dpi=dpi ,bbox_inches='tight',pad_inches = 0)\n",
        "\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    boxes = detections['detection_boxes']\n",
        "    scores = detections['detection_scores'],\n",
        "    clases_detected = detections['detection_classes']\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "\n",
        "\n",
        "    boxes = detections['detection_boxes']\n",
        "    scores = detections['detection_scores'],\n",
        "    clases_detected = detections['detection_classes']\n",
        "\n",
        "    print(boxes)\n",
        "    print('a')\n",
        "    print(scores)\n",
        "    print('b')\n",
        "    print(clases_detected)\n",
        "\n",
        "    #print(detections['detection_boxes'])\n",
        "    min_score_thresh = .3\n",
        "    true_boxes  = boxes[scores[0] > min_score_thresh]\n",
        "    true_scores = scores[0][scores[0] > min_score_thresh]\n",
        "    true_clases = clases_detected[scores[0] > min_score_thresh]\n",
        "\n",
        "    salida = []\n",
        "    salida.append(true_boxes.tolist())\n",
        "    salida.append(true_scores.tolist())\n",
        "    salida.append(true_clases.tolist())\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    width, height = image.size\n",
        "\n",
        "    salida.append(width)\n",
        "    salida.append(height)\n",
        "\n",
        "    return salida\n",
        "\n",
        "\n",
        "# sphinx_gallery_thumbnail_number = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj9FOjbIJBJu"
      },
      "source": [
        "make_checkpoint45('/content/159.jpg','95.jpg','/content/RESULTADOS/1.jpg',[2,3,4,6,8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAiXG79T_slO"
      },
      "source": [
        "create_dir('/content/RESULTADOS')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjiXiPGh0mXQ"
      },
      "source": [
        "import re\n",
        "imagenes_dir2 = '/content/FOTO'\n",
        "contenido2 = os.listdir(imagenes_dir2)\n",
        "imagenes_dir2 = []\n",
        "counter = 0\n",
        "result = []\n",
        "tiempos_procesar = []\n",
        "\n",
        "out = []\n",
        "concon = 0\n",
        "for fichero2 in contenido2:\n",
        "  print(concon)\n",
        "  concon = concon +1\n",
        "  x = re.search(\"[1-9]+[0-9]*\", fichero2)\n",
        "  id = x.group(0)\n",
        "  print(id)\n",
        "  image_path_gg = \"/content/FOTO/{}\".format(fichero2)\n",
        "\n",
        "  salida = make_checkpoint45(image_path_gg,fichero2,'/content/RESULTADOS/',[2,3,4,6,8])\n",
        "\n",
        "  result = []\n",
        "  converted_num = int(id)\n",
        "  converted_num = diccionario_ids[fichero2]\n",
        "  print(fichero2, \" \",converted_num)\n",
        "\n",
        "  width = salida[3]\n",
        "  height = salida[4]\n",
        "\n",
        "  boxes2 = []\n",
        "\n",
        "  for box in salida[0]:\n",
        "    ymin = int(box[0]*height)\n",
        "    xmin = int(box[1]*width)\n",
        "    ymax = int(box[2]*height)\n",
        "    xmax = int(box[3]*width)\n",
        "\n",
        "\n",
        "    box_new = []\n",
        "    box_new.append(xmin)\n",
        "    box_new.append(ymin)\n",
        "    box_new.append(xmax-xmin)\n",
        "    box_new.append(ymax-ymin)\n",
        "\n",
        "\n",
        "    boxes2.append(box_new)\n",
        "\n",
        "\n",
        "\n",
        "  result.extend(\n",
        "        [\n",
        "            {\n",
        "                \"image_id\": converted_num,\n",
        "                \"category_id\": salida[2][k]+1,\n",
        "                \"bbox\": box,\n",
        "                \"score\": salida[1][k],\n",
        "            }\n",
        "            for k, box in enumerate(boxes2)\n",
        "        ]\n",
        "  )\n",
        "\n",
        "  for sample in result:\n",
        "        out.append(sample)#detection result including\n",
        "\n",
        "with open('/content/test_data_MOD.json', 'w', encoding='utf-8') as file:\n",
        "  json.dump(out, file, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S1TJJRCHfU1"
      },
      "source": [
        "!zip -r '/content/RESULTADOSMOD.zip' '/content/RESULTADOS'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ2SbDXWEGHt"
      },
      "source": [
        "!zip -r '/content/RESULTADOSMODELO.zip' '/content/mimodelocompilado'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk_0QdEE0p4t"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import pylab\n",
        "import json\n",
        "\n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "\n",
        "\n",
        "annType = ['segm','bbox','keypoints']\n",
        "annType = annType[1]      #specify type here\n",
        "prefix = 'person_keypoints' if annType=='keypoints' else 'instances'\n",
        "print ('Running demo for *%s* results.'%(annType))\n",
        "\n",
        "# use the valadation labelme file\n",
        "# annFile = '/content/imagenesnormal.json'\n",
        "annFile = '/content/instances_default.json'\n",
        "cocoGt=COCO(annFile)\n",
        "#initialize COCO detections api\n",
        "# use the generated results\n",
        "resFile = '/content/test_data_MOD.json'\n",
        "cocoDt=cocoGt.loadRes(resFile)\n",
        "\n",
        "\n",
        "dts = json.load(open(resFile,'r'))\n",
        "imgIds = [imid['image_id'] for imid in dts]\n",
        "imgIds = sorted(list(set(imgIds)))\n",
        "\n",
        "\n",
        "'''\n",
        "imgIds=sorted(cocoGt.getImgIds())\n",
        "imgIds=imgIds[0:24]\n",
        "imgId = imgIds[np.random.randint(24)]\n",
        "'''\n",
        "\n",
        "# running box evaluation\n",
        "cocoEval = COCOeval(cocoGt,cocoDt,annType)\n",
        "\n",
        "\n",
        "cocoEval.params.imgIds  = imgIds\n",
        "\n",
        "cocoEval.params.catIds = [3] # 1 stands for the 'person' class, you can increase or decrease the category as needed\n",
        "\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA4iRjH-3DfF"
      },
      "source": [
        "create_dir('/content/RESULTADOS')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIk9W2As232H"
      },
      "source": [
        "make_checkpoint45('/content/1.jpg','1.jpg','/content/RESULTADOS/1.jpg',[2,3,4,6,8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74fWgteYG2yF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlwfeilrhM3O"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwE_703tEyi8"
      },
      "source": [
        "create_dir('/content/RESULTADOS')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heWloSVMEXWx"
      },
      "source": [
        "make_inference45('/content/1.jpg','92.jpg','/content/RESULTADOS/1.jpg',[2,3,4,6,8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoXqb27LY5dk"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "\n",
        "def make_checkpoint45(image_path,counter,image_save,etiquetas):\n",
        "\n",
        "    image_np = load_image_into_numpy_array(image_path)\n",
        "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "    detections = detect_fn(input_tensor)\n",
        "    num_detections = int(detections.pop('num_detections'))\n",
        "    detections = {key: value[0, :num_detections].numpy()\n",
        "                  for key, value in detections.items()}\n",
        "    detections['num_detections'] = num_detections\n",
        "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "    image_np_with_detections = image_np.copy()\n",
        "\n",
        "    boxes_filter = []\n",
        "    scores_filter = []\n",
        "    classes_filter = []\n",
        "\n",
        "    for item in range(len(detections['detection_boxes'])):\n",
        "      if detections['detection_classes'][item] in etiquetas:\n",
        "        boxes_filter.append(detections['detection_boxes'][item].tolist())\n",
        "        scores_filter.append(detections['detection_scores'][item])\n",
        "        classes_filter.append(detections['detection_classes'][item])\n",
        "\n",
        "\n",
        "    detections['detection_boxes'] = np.array(boxes_filter)\n",
        "    detections['detection_classes'] = np.array(classes_filter)\n",
        "    detections['detection_scores'] = np.array(scores_filter)\n",
        "\n",
        "    print('ANTES DEL IOU TENGO '+str(len(detections['detection_boxes'])))\n",
        "\n",
        "\n",
        "    min_score_thresh=.3\n",
        "    boxes = detections['detection_boxes']\n",
        "    scores = detections['detection_scores'],\n",
        "    clases_detected = detections['detection_classes']\n",
        "\n",
        "    true_boxes  = boxes[scores[0] > min_score_thresh]\n",
        "    true_scores = scores[0][scores[0] > min_score_thresh]\n",
        "    true_clases = clases_detected[scores[0] > min_score_thresh]\n",
        "\n",
        "    new_counter = 0\n",
        "    img = cv2.imread(image_path)\n",
        "    im = Image.open(image_path)\n",
        "\n",
        "    contador_final = 0\n",
        "\n",
        "    img__READ = cv2.imread(image_path)\n",
        "    height, width, channels = img__READ.shape\n",
        "\n",
        "    diccionario_final_predicciones = []\n",
        "    diccionario_final_scores = []\n",
        "    diccionario_final_clases = []\n",
        "\n",
        "    for i in range(len(true_boxes)):\n",
        "        box_detected = true_boxes[i]\n",
        "        ymin = int(box_detected[0]*height*2)\n",
        "        xmin = int(box_detected[1]*width*2)\n",
        "        ymax = int(box_detected[2]*height*2)\n",
        "        xmax = int(box_detected[3]*width*2)\n",
        "\n",
        "        ymin11 = (box_detected[0])\n",
        "        xmin11 = (box_detected[1])\n",
        "        ymax11 = (box_detected[2])\n",
        "        xmax11 = (box_detected[3])\n",
        "\n",
        "        coordenadas_good = []\n",
        "        coordenadas_good.append(ymin11)\n",
        "        coordenadas_good.append(xmin11)\n",
        "        coordenadas_good.append(ymax11)\n",
        "        coordenadas_good.append(xmax11)\n",
        "\n",
        "        diccionario_final_predicciones.append(coordenadas_good)\n",
        "        diccionario_final_scores.append(true_scores[new_counter])\n",
        "        diccionario_final_clases.append(true_clases[new_counter])\n",
        "\n",
        "        new_counter = new_counter+1\n",
        "\n",
        "\n",
        "    detections['detection_boxes'] = np.array(diccionario_final_predicciones)\n",
        "    detections['detection_classes'] = diccionario_final_clases\n",
        "    detections['detection_scores'] = diccionario_final_scores\n",
        "\n",
        "\n",
        "    diccionario_final2_predicciones = []\n",
        "    diccionario_final2_scores = []\n",
        "    diccionario_final2_clases = []\n",
        "\n",
        "    puntero = 0\n",
        "    error = [-1,-1,-1,-1]\n",
        "\n",
        "\n",
        "\n",
        "    '''LLEGADOS A ESTE PUNTO TENGO QUE HACER IOU'''\n",
        "\n",
        "    for ia in range(len(diccionario_final_predicciones)):\n",
        "      coordenadas_A = diccionario_final_predicciones[ia]\n",
        "\n",
        "      if diccionario_final_predicciones[ia][0]!=-1:\n",
        "\n",
        "        diccionario_final2_predicciones.append(diccionario_final_predicciones[ia])\n",
        "        diccionario_final2_scores.append(diccionario_final_scores[ia])\n",
        "        diccionario_final2_clases.append(diccionario_final_clases[ia])\n",
        "\n",
        "\n",
        "        for ib in range(len(diccionario_final_predicciones)):\n",
        "          if ia != ib:\n",
        "\n",
        "            coordenadas_B = diccionario_final_predicciones[ib]\n",
        "\n",
        "            if diccionario_final_predicciones[ib][0]!=-1:\n",
        "                yminA = max(coordenadas_A[0]*height,coordenadas_B[0]*height)\n",
        "                xminA = max(coordenadas_A[1]*width,coordenadas_B[1]*width)\n",
        "                ymaxA = min(coordenadas_A[2]*height,coordenadas_B[2]*height)\n",
        "                xmaxA = min(coordenadas_A[3]*width,coordenadas_B[3]*width)\n",
        "\n",
        "                interArea = max(0, ymaxA - yminA + 1) * max(0, xmaxA - xminA + 1)\n",
        "\n",
        "                boxAArea = (coordenadas_A[2]*height - coordenadas_A[0]*height + 1) * (coordenadas_A[3]*width - coordenadas_A[1]*width + 1)\n",
        "                boxBArea = (coordenadas_B[2]*height - coordenadas_B[0]*height + 1) * (coordenadas_B[3]*width - coordenadas_B[1]*width + 1)\n",
        "\n",
        "                iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "                if iou > 0.1:\n",
        "                    contador = len(diccionario_final2_predicciones)-1\n",
        "                    if (diccionario_final2_scores[contador]) < diccionario_final_scores[ib]:\n",
        "\n",
        "                      contador = len(diccionario_final2_predicciones)-1\n",
        "                      diccionario_final2_predicciones[contador] = diccionario_final_predicciones[ib]\n",
        "                      diccionario_final2_scores[contador] = diccionario_final_scores[ib]\n",
        "                      diccionario_final2_clases[contador] = diccionario_final_clases[ib]\n",
        "\n",
        "                    diccionario_final_predicciones[ib] = error\n",
        "\n",
        "    detections['detection_boxes'] = np.array(diccionario_final2_predicciones)\n",
        "    detections['detection_classes'] = diccionario_final2_clases\n",
        "    detections['detection_scores'] = diccionario_final2_scores\n",
        "\n",
        "\n",
        "    boxes_filter = []\n",
        "    scores_filter = []\n",
        "    classes_filter = []\n",
        "\n",
        "    for item in range(len(detections['detection_boxes'])):\n",
        "      if detections['detection_classes'][item] in etiquetas:\n",
        "        boxes_filter.append(detections['detection_boxes'][item].tolist())\n",
        "        scores_filter.append(detections['detection_scores'][item])\n",
        "        classes_filter.append(detections['detection_classes'][item])\n",
        "\n",
        "\n",
        "    detections['detection_boxes'] = np.array(boxes_filter)\n",
        "    detections['detection_classes'] = np.array(classes_filter)\n",
        "    detections['detection_scores'] = np.array(scores_filter)\n",
        "\n",
        "\n",
        "\n",
        "    print('DESPUES DEL IOU TENGO '+str(len(detections['detection_boxes'])))\n",
        "\n",
        "\n",
        "\n",
        "    label_id_offset = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_detections,\n",
        "        detections['detection_boxes'],\n",
        "        detections['detection_classes']+label_id_offset,\n",
        "        detections['detection_scores'],\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        max_boxes_to_draw=200,\n",
        "        min_score_thresh=.3,\n",
        "        agnostic_mode=False,\n",
        "        line_thickness=1)\n",
        "    plt.figure()\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "    plt.imshow(image_np_with_detections)\n",
        "    nombre = counter\n",
        "    plt.savefig(image_save+nombre,  dpi=dpi ,bbox_inches='tight',pad_inches = 0)\n",
        "\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    boxes = detections['detection_boxes']\n",
        "    scores = detections['detection_scores'],\n",
        "    clases_detected = detections['detection_classes']\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "\n",
        "\n",
        "    boxes = detections['detection_boxes']\n",
        "    scores = detections['detection_scores'],\n",
        "    clases_detected = detections['detection_classes']\n",
        "\n",
        "    print(boxes)\n",
        "    print('a')\n",
        "    print(scores)\n",
        "    print('b')\n",
        "    print(clases_detected)\n",
        "\n",
        "    #print(detections['detection_boxes'])\n",
        "    min_score_thresh = .3\n",
        "    true_boxes  = boxes[scores[0] > min_score_thresh]\n",
        "    true_scores = scores[0][scores[0] > min_score_thresh]\n",
        "    true_clases = clases_detected[scores[0] > min_score_thresh]\n",
        "\n",
        "    salida = []\n",
        "    salida.append(true_boxes.tolist())\n",
        "    salida.append(true_scores.tolist())\n",
        "    salida.append(true_clases.tolist())\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    width, height = image.size\n",
        "\n",
        "    salida.append(width)\n",
        "    salida.append(height)\n",
        "\n",
        "    return salida\n",
        "\n",
        "\n",
        "# sphinx_gallery_thumbnail_number = 2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}